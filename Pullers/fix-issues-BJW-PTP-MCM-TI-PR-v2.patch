diff --git a/Dambooru2024_local_puller.py b/Dambooru2024_local_puller.py
--- a/Dambooru2024_local_puller.py
+++ b/Dambooru2024_local_puller.py
@@ -130,6 +130,14 @@
     memory_low_water_gb: int = 40 # Resume processing below this memory usage
     max_pending_batches: int = 5  # Maximum tar batches to keep in memory
     enable_fsync: bool = False  # Disable fsync for speed (filesystem will handle it)
+    # Large tar caching policy: "skip" (default), "mmap", or "full"
+    large_tars_policy: str = "skip"
+    # Batched JSON writer flush cadence (seconds)
+    json_flush_interval: float = 5.0
+    # Collision policy for tar basenames when multiple directories contain same basename:
+    # "strict" (default: require full path) or "prefer_top" (prefer top-level or shortest path)
+    on_basename_collision: str = "strict"
+
 
 # ---------------------------------------------------------------------------
 # Directory Sharding
@@ -388,7 +396,7 @@
 class MemoryCacheManager:
     """Manages in-memory caching of tar files for fast access."""
     
-    def __init__(self, max_size_gb: int = 20, read_buffer_size_mb: int = 256, use_memory_mapping: bool = False):
+    def __init__(self, max_size_gb: int = 20, read_buffer_size_mb: int = 256, use_memory_mapping: bool = False, large_tars_policy: str = "skip"):
         self.max_size_bytes = max_size_gb * 1024 * 1024 * 1024
         self.read_buffer_size_mb = read_buffer_size_mb
         self.cache: Dict[str, bytes] = {}
@@ -402,6 +410,7 @@
         self.prefetch_threads = 2
         self.mmap_handles: Dict[str, mmap.mmap] = {}  # Memory mapped files
         self.use_memory_mapping = use_memory_mapping
+        self.large_tar_policy = large_tars_policy
         self._closed = False
         self.prefetch_threads_list = []
         self._start_prefetch_worker()
@@ -506,10 +515,21 @@
         try:
             file_size = tar_path.stat().st_size
             
-            # Skip full-cache of huge tars when not using mmap
-            if file_size > 2 * 1024**3 and not self.use_memory_mapping:
-                logging.info(f"⏭️ Skipping full-cache of large tar ({file_size/1024**3:.1f}GB): {tar_name}")
-                return None
+            # Large tar handling policy
+            if file_size > 2 * 1024**3:
+                if self.large_tar_policy == "mmap":
+                    mm = self._try_memory_map(tar_path)
+                    if mm:
+                        load_event = threading.Event()
+                        with self.lock:
+                            if tar_name in self.cache or tar_name in self.mmap_handles:
+                                return None
+                            self.mmap_handles[tar_name] = mm
+                            self.access_times[tar_name] = time.time()
+                        return None
+                elif self.large_tar_policy == "skip" and not self.use_memory_mapping:
+                    logging.info(f"⏭️ Skipping full-cache of large tar ({file_size/1024**3:.1f}GB): {tar_name}")
+                    return None
             
             # Check if we have space
             if file_size > self.max_size_bytes:
@@ -538,8 +558,15 @@
             else:
                 buffer_size = self.read_buffer_size_mb * 1024 * 1024
             
+            # Chunked streaming to avoid transient RSS spikes
+            data_ba = bytearray()
             with open(tar_path, 'rb', buffering=buffer_size) as f:
-                data = f.read()
+                while True:
+                    chunk = f.read(buffer_size)
+                    if not chunk:
+                        break
+                    data_ba.extend(chunk)
+            data = bytes(data_ba)
             
             with self.lock:
                 self.cache[tar_name] = data
@@ -688,7 +715,7 @@
         self.progress_tracker = progress_tracker
         self.json_writer = json_writer
         self.cache_manager = MemoryCacheManager(
-            cfg.max_memory_cache_gb, cfg.read_buffer_size_mb, cfg.use_memory_mapping
+            cfg.max_memory_cache_gb, cfg.read_buffer_size_mb, cfg.use_memory_mapping, cfg.large_tars_policy
         )
         self.memory_monitor = MemoryMonitor(cfg.memory_high_water_gb, cfg.memory_low_water_gb)
 
@@ -898,10 +925,10 @@
                 # Build member index keyed by BASENAME -> list[TarInfo]
                 from collections import defaultdict as _dd
                 members_dict = _dd(list)
-                # Only index members we need
-                needed_files = {filename for _, filename, _, _ in batch}
+                # Only index members by BASENAME we need
+                needed_files = {fn.rsplit("/", 1)[-1] for _, fn, _, _ in batch}
                 for member in tar.getmembers():
-                    base_name = member.name.rsplit('/', 1)[-1]
+                    base_name = member.name.rsplit("/", 1)[-1]
                     if base_name in needed_files:
                         members_dict[base_name].append(member)
 
@@ -949,9 +976,27 @@
         
         for image_id, filename, dest_path, row_data in chunk:
             try:
-                # Find member by BASENAME (handle collisions by picking first)
-                candidates = members_dict.get(filename, [])
-                member = candidates[0] if candidates else None
+                base = filename.rsplit("/", 1)[-1]
+                candidates = members_dict.get(base, [])
+                member = None
+                # If full path was provided, prefer exact match among candidates
+                for cand in candidates:
+                    if cand.name == filename:
+                        member = cand
+                        break
+                if member is None:
+                    if len(candidates) == 1:
+                        member = candidates[0]
+                    elif len(candidates) > 1:
+                        if self.cfg.on_basename_collision == "prefer_top":
+                            top = [c for c in candidates if "/" not in c.name]
+                            if len(top) == 1:
+                                member = top[0]
+                            else:
+                                member = min(candidates, key=lambda c: c.name.count("/"))
+                        else:
+                            logging.debug(f"Ambiguous basename {base}, skipping (provide full path).")
+                            member = None
                 
                 if not member:
                     logging.debug(f"Member {filename} not found in tar")
@@ -1196,7 +1241,7 @@
     
     # Initialize components
     sharding = DirectorySharding(dest_dir, cfg.files_per_shard)
-    json_writer = BatchedJSONWriter(flush_interval=5.0, batch_size=1000, enable_fsync=cfg.enable_fsync) if cfg.per_image_json else None
+    json_writer = BatchedJSONWriter(flush_interval=cfg.json_flush_interval, batch_size=1000, enable_fsync=cfg.enable_fsync) if cfg.per_image_json else None
     progress_tracker = ValidatingProgressTracker(dest_dir / "progress.json", update_interval=1000)
     
     # Validation
