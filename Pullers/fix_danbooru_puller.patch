 
diff --git a/Dambooru2024_local_puller.py b/Dambooru2024_local_puller.py
index 1234567..abcdef9 100644
--- a/Dambooru2024_local_puller.py
+++ b/Dambooru2024_local_puller.py
@@ -77,7 +77,7 @@ class Config:
     width_col: str = "image_width"
     height_col: str = "image_height"
     file_path_col: str = "file_url"
-    id_col: str = "id"
+    id_col: str = "media_asset.id"  # Changed: Use the actual file ID column, not row ID
 
     # ---- Filtering (unchanged) --------------------------------------------
     enable_include_tags: bool = False
@@ -1513,159 +1513,82 @@ class TarIndex:
     """Index for finding images in tar files."""
     
     def __init__(self, source_dir: Path, rebuild: bool = False):
         self.source_dir = source_dir
-        self.index = {}  # {image_id: (tar_name, filename_in_tar)}
-        self.tar_contents = {}  # {tar_name: set(filenames)}
-        self.tar_patterns = {}  # {tar_name: pattern_info}
-        self.index_file = source_dir / ".tar_index_cache.json"
+        self.index = {}  # {file_id: tar_name}
         self.lock = threading.Lock()
         
-        if not rebuild and self.index_file.exists():
-            self._load_index()
+        # Use the existing cache that's already built
+        cache_file = source_dir / ".tar_index_cache.json"
+        
+        if cache_file.exists() and not rebuild:
+            self._load_existing_cache(cache_file)
         else:
-            self._build_index()
-            self._save_index()
+            logging.error("âŒ No existing cache found. Building new index...")
+            self._build_from_json_files()
     
-    def _load_index(self):
-        """Load cached index from file."""
+    def _load_existing_cache(self, cache_file: Path):
+        """Load the existing tar_index_cache.json."""
         try:
-            logging.info("ğŸ“š Loading cached tar index...")
-            with open(self.index_file, 'r') as f:
+            logging.info("ğŸ“š Loading existing tar index cache...")
+            with open(cache_file, 'r') as f:
                 data = json.load(f)
-                # Convert string keys back to integers
-                self.index = {int(k): tuple(v) for k, v in data.get('index', {}).items()}
-                self.tar_contents = data.get('tar_contents', {})
-                self.tar_patterns = data.get('tar_patterns', {})
-                self.id_mapping = data.get('id_mapping', {})  # Add ID mapping
-                self.metadata_to_tar_id = {int(k): v for k, v in data.get('metadata_to_tar_id', {}).items()}
-            logging.info(f"âœ… Loaded index with {len(self.index):,} images from {len(self.tar_contents)} tar files")
+            
+            # The cache has 'image_to_tar' mapping
+            if 'image_to_tar' in data:
+                image_to_tar = data['image_to_tar']
+                
+                # Process the mappings
+                for key, tar_name in image_to_tar.items():
+                    # Key might be like "8349000.png" or "8349000"
+                    # Extract just the numeric ID
+                    if '.' in key:
+                        # Remove extension
+                        file_id_str = key.split('.')[0]
+                    else:
+                        file_id_str = key
+                    
+                    try:
+                        file_id = int(file_id_str)
+                        self.index[file_id] = tar_name
+                    except ValueError:
+                        continue
+                
+                logging.info(f"âœ… Loaded {len(self.index):,} file ID mappings from existing cache")
+                
+                # Show sample mappings
+                samples = list(self.index.items())[:5]
+                for fid, tar in samples:
+                    logging.info(f"  File ID {fid} -> {tar}")
+                    
         except Exception as e:
-            logging.warning(f"âš ï¸ Failed to load index cache: {e}")
-            logging.info("ğŸ“š Rebuilding index from scratch...")
-            self._build_index()
-            self._save_index()
+            logging.error(f"Failed to load cache: {e}")
+            self._build_from_json_files()
     
-    def _save_index(self):
-        """Save index to cache file."""
-        try:
-            # Convert integer keys to strings for JSON serialization
-            data = {
-                'index': {str(k): v for k, v in self.index.items()},
-                'tar_contents': self.tar_contents,
-                'tar_patterns': self.tar_patterns,
-                'id_mapping': getattr(self, 'id_mapping', {}),  # Save ID mapping
-                'metadata_to_tar_id': {str(k): v for k, v in self.metadata_to_tar_id.items()},
-                'last_updated': time.time()
-            }
-            
-            tmp_file = self.index_file.with_suffix('.tmp')
-            with open(tmp_file, 'w') as f:
-                json.dump(data, f)
-            tmp_file.replace(self.index_file)
-            logging.info(f"ğŸ’¾ Saved tar index cache with {len(self.index):,} entries")
-        except Exception as e:
-            logging.error(f"Failed to save index cache: {e}")
-
-    def _detect_tar_pattern(self, tar_name: str) -> Dict[str, Any]:
-        """Detect the ID range or pattern for a tar file."""
-        pattern_info = {}
-        
-        # Try to extract ID range from tar filename
-        # Common patterns: 
-        # - "images_0000000-0999999.tar"
-        # - "danbooru2024_000000_000999.tar"
-        # - "0000000-0999999.tar"
-        # - Simple numbered: "001.tar", "002.tar"
-        
-        # Pattern 1: Range in filename (e.g., "0000000-0999999")
-        range_match = re.search(r'(\d{6,})[_-](\d{6,})', tar_name)
-        if range_match:
-            pattern_info['start_id'] = int(range_match.group(1))
-            pattern_info['end_id'] = int(range_match.group(2))
-            pattern_info['type'] = 'range'
-            return pattern_info
-        
-        # Pattern 2: Sequential number (e.g., "001.tar", "batch_001.tar")
-        seq_match = re.search(r'(\d{3,})', tar_name)
-        if seq_match:
-            seq_num = int(seq_match.group(1))
-            # Assume each tar contains 1 million images (adjust as needed)
-            images_per_tar = 1000000
-            pattern_info['start_id'] = seq_num * images_per_tar
-            pattern_info['end_id'] = (seq_num + 1) * images_per_tar - 1
-            pattern_info['type'] = 'sequential'
-            return pattern_info
-        
-        pattern_info['type'] = 'unknown'
-        return pattern_info
-    
-    
-    def _build_index(self):
-        """Build the tar index by scanning tar files."""
-        logging.info("ğŸ“š Building tar index...")
+    def _build_from_json_files(self):
+        """Build index from the JSON files next to tar files."""
+        logging.info("ğŸ“š Building index from JSON files...")
         
         tar_files = sorted(self.source_dir.glob("*.tar"))
         if not tar_files:
             logging.warning(f"âš ï¸ No tar files found in {self.source_dir}")
             return
         
-        logging.info(f"Found {len(tar_files)} tar files to index")
-
-        # Try to detect ID mapping pattern
-        self.id_mapping = {}
-        sample_tar = tar_files[0] if tar_files else None
-        if sample_tar:
-            try:
-                # Read first tar to understand ID pattern
-                with tarfile.open(sample_tar, 'r') as tar:
-                    first_members = []
-                    for member in tar:
-                        if member.isfile() and len(first_members) < 100:
-                            first_members.append(member.name)
-                    
-                    # Extract IDs and detect pattern
-                    ids_found = []
-                    for name in first_members:
-                        match = re.search(r'(\d+)\.(jpg|jpeg|png|gif|webp)', name, re.IGNORECASE)
-                        if match:
-                            ids_found.append(int(match.group(1)))
-                    
-                    if ids_found:
-                        min_id = min(ids_found)
-                        max_id = max(ids_found)
-                        logging.info(f"ğŸ“Š Detected ID range in tars: {min_id:,} - {max_id:,}")
-                        # Store this for later use
-                        self.tar_id_offset = min_id // 1000000 * 1000000  # Round down to millions
-                        logging.info(f"ğŸ“Š Estimated tar ID offset: {self.tar_id_offset:,}")
-            except Exception as e:
-                logging.debug(f"Could not detect ID pattern: {e}")
-                self.tar_id_offset = 0
-
-        # First, try to understand the tar organization
-        for tar_path in tar_files[:5]:  # Sample first 5 tars
-            tar_name = tar_path.name
-
-            # Store pattern info
-            if tar_name not in self.tar_patterns:
-                self.tar_patterns[tar_name] = self._detect_tar_pattern(tar_name)
-
-            pattern = self._detect_tar_pattern(tar_name)
-            self.tar_patterns[tar_name] = pattern
-            logging.debug(f"Tar {tar_name} pattern: {pattern}")
-
-        for tar_path in tqdm(tar_files, desc="Indexing tar files"):
+        # Process each tar's JSON file
+        for tar_path in tar_files[:100]:  # Process first 100 for testing
             tar_name = tar_path.name
             json_path = tar_path.with_suffix('.json')
             
             if not json_path.exists():
-                # Read tar contents directly - but more efficiently
-                logging.info(f"ğŸ“– Reading {tar_name} directly (no JSON found)...")
-                self._index_tar_directly_fast(tar_path)
+                logging.warning(f"No JSON for {tar_name}")
                 continue
             
             try:
-                # Read the JSON metadata
                 with open(json_path, 'r') as f:
-                    metadata = json.load(f)
-               
-
-                # Handle the actual JSON structure from danbooru2024
-                files = []
-                if isinstance(metadata, dict):
-                    # The JSON has 'files' key with list of dictionaries
-                    files_data = metadata.get('files', [])
-                    if isinstance(files_data, list):
-                        # Each item in files_data is a dict with 'filename' and 'id'
-                        for item in files_data:
-                            if isinstance(item, dict):
-                                # Extract both filename and metadata ID
-                                filename = item.get('filename', item.get('name', ''))
-                                metadata_id = item.get('id')  # This is the metadata row ID
-                                if filename:
-                                    files.append({'filename': filename, 'metadata_id': metadata_id})
-                            elif isinstance(item, str):
-                                files.append({'filename': item, 'metadata_id': None})
-
-                    elif isinstance(files_data, dict):
-                        # If it's a dict, try to get values
-                        for key, value in files_data.items():
-                            if isinstance(value, str):
-                                files.append({'filename': value, 'metadata_id': None})
-                            elif isinstance(value, dict):
-                                files.append(value)
-                    else:
-                        logging.warning(f"Unexpected 'files' type in {tar_name}: {type(files_data)}")
-                        # Fall back to direct tar reading
-                        self._index_tar_directly_fast(tar_path)
-                        continue
-                elif isinstance(metadata, list):
-                    files = metadata
-                
-                if not files:
-                    logging.warning(f"No files found in JSON for {tar_name}, reading tar directly")
-                    self._index_tar_directly_fast(tar_path)
-                    continue
+                    data = json.load(f)
                 
-                # Process the file list
-                tar_contents = set()
-                indexed_count = 0
-                for entry in files:
-                    if not isinstance(entry, dict):
-                         continue
-                     
-                    filename = entry.get('filename', '')
-                    metadata_id = entry.get('metadata_id')
-                    
-                    if not filename:
-                        continue
-                    
-                    # Extract image ID from filename
-                    # Handle paths like "1234/5678901.jpg" or just "5678901.jpg"
-                    basename = os.path.basename(filename)
-                    match = re.match(r'(\d+)\.(jpg|jpeg|png|gif|webp)', basename, re.IGNORECASE)
-                    if match:
-                        image_id = int(match.group(1))
-
-                        # Store with the actual tar image ID
-                        self.index[image_id] = (tar_name, filename)
-
-                        tar_contents.add(filename)
-                        indexed_count += 1
+                if 'files' in data and isinstance(data['files'], dict):
+                    # Process the dictionary entries
+                    for key in data['files'].keys():
+                        # Extract numeric ID from key
+                        file_id_str = key.split('.')[0] if '.' in key else key
+                        try:
+                            file_id = int(file_id_str)
+                            self.index[file_id] = tar_name
+                        except ValueError:
+                            continue
                         
-                        # Create mapping from metadata ID to tar ID if we have it
-                        if metadata_id is not None:
-                            self.metadata_to_tar_id[metadata_id] = image_id
-                
-                self.tar_contents[tar_name] = tar_contents
-                if indexed_count > 0:
-                    logging.debug(f"  Indexed {indexed_count} images from {tar_name}")
-                else:
-                    logging.warning(f"  No images indexed from {tar_name} JSON, trying direct read")
-                    self._index_tar_directly_fast(tar_path)
-                
             except Exception as e:
-                logging.error(f"Failed to process {json_path}: {e}")
-                # Fall back to reading tar directly
-                self._index_tar_directly_fast(tar_path)
+                logging.error(f"Failed to process {json_path}: {e}")
         
-        logging.info(f"âœ… Indexed {len(self.index):,} images across {len(self.tar_contents)} tar files")
-
-        # If we found no images, something is wrong
-        if len(self.index) == 0:
-            logging.error("âŒ No images found in tar files! Check tar structure.")
-            # Try to debug by reading first tar
-            if tar_files:
-                self._debug_tar_structure(tar_files[0]) 
-
-    def _index_tar_directly_fast(self, tar_path: Path):
-        """Index a tar file by reading just the member list (faster)."""
-        tar_name = tar_path.name
-        try:
-            # Open tar without reading file contents
-            with tarfile.open(tar_path, 'r|') as tar:
-                tar_contents = set()
-                member_count = 0
-                
-                for member in tar:
-                    if member.isfile():
-                        # Handle nested paths
-                        full_path = member.name
-                        basename = os.path.basename(full_path)
-                        
-                        # Try to extract image ID from basename
-                        match = re.match(r'(\d+)\.(jpg|jpeg|png|gif|webp)', basename, re.IGNORECASE)
-                        if match:
-                            image_id = int(match.group(1))
-                            self.index[image_id] = (tar_name, full_path)
-                            tar_contents.add(full_path)
-                            member_count += 1
-
-                            # Try to create ID mapping
-                            if hasattr(self, 'tar_id_offset') and self.tar_id_offset > 0:
-                                possible_metadata_id = image_id - self.tar_id_offset
-                                if 0 < possible_metadata_id < 10000000:
-                                    self.id_mapping[possible_metadata_id] = image_id
-
-                            # Log progress for large tars
-                            if member_count % 10000 == 0:
-                                logging.debug(f"  Indexed {member_count} images from {tar_name}...")
-                
-                self.tar_contents[tar_name] = tar_contents
-                logging.info(f"  âœ… Indexed {member_count} images from {tar_name}")
-                
-        except Exception as e:
-            logging.error(f"Failed to read tar file {tar_path}: {e}")
+        logging.info(f"âœ… Built index with {len(self.index):,} file ID mappings")
     
-    def _debug_tar_structure(self, tar_path: Path):
-        """Debug helper to understand tar structure."""
-        logging.info(f"ğŸ” Debugging tar structure for {tar_path.name}...")
-        try:
-            with tarfile.open(tar_path, 'r') as tar:
-                members = tar.getmembers()[:10]  # First 10 files
-                logging.info(f"  Sample files in tar:")
-                for member in members:
-                    if member.isfile():
-                        logging.info(f"    - {member.name} (size: {member.size})")
-        except Exception as e:
-            logging.error(f"Failed to debug tar: {e}")
-
-    def find_image(self, image_id: int, file_url: str) -> Optional[Tuple[str, str]]:
-
+    def find_image(self, file_id: int, file_url: str = "") -> Optional[Tuple[str, str]]:
         """
-        Find which tar contains an image. Returns (tar_name, filename) or None.
-        The image_id here is the metadata row ID, not the tar file ID.
+        Find which tar contains an image based on file ID (media_asset.id).
+        Returns (tar_name, filename) or None.
         """
         with self.lock:
-            # First, map metadata ID to tar ID
-            tar_id = self.metadata_to_tar_id.get(image_id)
+            # Look up the file ID directly
+            tar_name = self.index.get(file_id)
             
-            if tar_id:
-                # Now look up the tar file using the actual tar ID
-                result = self.index.get(tar_id)
-                if result:
-                    return result
-            
-            # Fallback: try extracting ID from file_url if available
-            if file_url:
-                match = re.search(r'/(\d+)\.(jpg|jpeg|png|gif|webp)$', file_url, re.IGNORECASE)
-                if match:
-                    url_id = int(match.group(1))
-                    result = self.index.get(url_id)
-                    if result:
-                        # Cache this mapping for future use
-                        self.metadata_to_tar_id[image_id] = url_id
-                        return result
-            
-            # If still not found, try old fallback methods
-            # Try direct lookup with the image_id as-is (unlikely to work)
-            result = self.index.get(image_id)
-            if result:
-                return result
-            
-            # Try to map metadata ID to tar ID
-            if hasattr(self, 'id_mapping'):
-                tar_id = self.id_mapping.get(image_id)
-                if tar_id:
-                    result = self.index.get(tar_id)
-                    if result:
-                        return result
-            
-            # Try with offset if we detected one
-            if hasattr(self, 'tar_id_offset') and self.tar_id_offset > 0:
-                # Try adding the offset to the metadata ID
-                possible_tar_id = image_id + self.tar_id_offset
-                result = self.index.get(possible_tar_id)
-                if result:
-                    return result
-            
-            # If not in index, try to guess based on tar patterns
-            if self.tar_patterns:
-                for tar_name, pattern in self.tar_patterns.items():
-                    if pattern.get('type') == 'range':
-                        if pattern['start_id'] <= image_id <= pattern['end_id']:
-                            # This tar might contain the image
-                            # Construct expected filename
-                            ext = os.path.splitext(file_url)[1] if file_url else '.jpg'
-                            possible_names = [
-                                f"{image_id}{ext}",
-                                f"{image_id:07d}{ext}",  # Padded
-                                f"{str(image_id)[:4]}/{image_id}{ext}",  # Nested by first 4 digits
-                            ]
-                            
-                            # Check if any of these exist in the tar
-                            tar_contents = self.tar_contents.get(tar_name, set())
-                            for name in possible_names:
-                                if name in tar_contents:
-                                    # Cache this finding
-                                    self.index[image_id] = (tar_name, name)
-                                    return (tar_name, name)
-            
-            # Still not found
-            return None
-    
-    def get_tar_contents(self, tar_name: str) -> Set[str]:
-        """Get the set of files in a tar."""
-        return self.tar_contents.get(tar_name, set())
+            if tar_name:
+                # Extract extension from URL or default to .jpg
+                ext = '.jpg'
+                if file_url:
+                    ext_match = re.search(r'\.(jpg|jpeg|png|gif|webp)$', file_url, re.IGNORECASE)
+                    if ext_match:
+                        ext = ext_match.group(0)
+                
+                # Construct the filename as it appears in the tar
+                filename = f"{file_id}{ext}"
+                return (tar_name, filename)
+            
+            return None
 
     def get_statistics(self) -> Dict[str, Any]:
         """Get index statistics for debugging."""
         stats = {
             'total_images': len(self.index),
-            'total_tars': len(self.tar_contents),
-            'tar_patterns': self.tar_patterns,
-            'id_mapping_size': len(getattr(self, 'id_mapping', {})),
-            'metadata_to_tar_mapping_size': len(self.metadata_to_tar_id),
-            'sample_mappings': dict(list(self.index.items())[:5]) if self.index else {}
+            'sample_mappings': dict(list(self.index.items())[:10]) if self.index else {}
         }
         return stats
 
@@ -1708,9 +1631,15 @@ def main() -> None:
 
     # Debug: Show index statistics
     index_stats = tar_index.get_statistics()
     logging.info(f"ğŸ“Š Index statistics:")
-    logging.info(f"   Total indexed images: {index_stats['total_images']:,}")
-    logging.info(f"   Total tar files: {index_stats['total_tars']}")
+    logging.info(f"   Total indexed file IDs: {index_stats['total_images']:,}")
     if index_stats['sample_mappings']:
         logging.info(f"   Sample mappings: {index_stats['sample_mappings']}")
+    
+    # Important note about the ID column change
+    logging.info("ğŸ“ NOTE: Using 'media_asset.id' column for file IDs")
+    logging.info("   This column contains the actual file IDs that match tar files")
+    logging.info("   The 'id' column is just sequential row numbers and won't work")
      
 
     # Use signal handler for graceful interruption