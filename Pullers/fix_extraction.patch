--- a/Dambooru2024_local.py
+++ b/Dambooru2024_local.py
@@ -938,23 +938,32 @@ def stream_filtered_metadata(path: Path, cfg: Config, stop_handler: Optional[So
     lf = lf.filter(filter_expr)
     
     try:
-        logging.info(f"🔥 Starting streaming collection with batch size {cfg.batch_size}")
-        # Try different collection strategies
+        logging.info(f"🔥 Starting collection with batch size {cfg.batch_size}")
         df = None
+        
+        # Try different collection strategies with proper error handling
         try:
-            # First try: streaming collection
-            df = lf.collect(streaming=True)
-            logging.info("✅ Using streaming collection")
+            # First try: new engine API for Polars 1.25.0+
+            df = lf.collect(engine="streaming")
+            logging.info("✅ Using streaming engine")
         except Exception as e1:
-            logging.warning(f"⚠️ Streaming collection failed: {e1}")
+            if "unexpected keyword argument 'engine'" in str(e1):
+                # Fallback for older Polars versions
+                try:
+                    df = lf.collect(streaming=True)
+                    logging.info("✅ Using legacy streaming parameter")
+                except Exception as e1b:
+                    logging.warning(f"⚠️ Streaming collection failed: {e1b}")
+                    df = None
+            else:
+                logging.warning(f"⚠️ Streaming engine failed: {e1}")
+                df = None
+        
+        if df is None:
+            # Second try: regular collection
             try:
-                # Second try: regular collection
                 logging.info("🔄 Trying regular collection...")
                 df = lf.collect()
                 logging.info("✅ Using regular collection")
             except Exception as e2:
                 logging.warning(f"⚠️ Regular collection failed: {e2}")
                 # Final fallback: read directly with chunking
@@ -1090,7 +1099,6 @@ class TarIndex:
         logging.info(f"✅ Built index with {len(self.index):,} file ID mappings from {len(tar_files)} tar files")
 
         # Show sample mappings for debugging
         if self.index:
             sample = list(self.index.items())[:5]
             logging.info("📋 Sample ID mappings:")
@@ -1106,8 +1114,6 @@ class TarIndex:
                 internal_path = self.index_paths.get(image_id, f"{image_id}.jpg")
                 return (tar_name, internal_path)
 
-        # Log miss for debugging
         if len(self.index) > 0:
             sample_ids = list(self.index.keys())[:3]
             logging.debug(f"❌ ID {image_id} not in index. Sample IDs: {sample_ids}")
 
         return None
 
-    # cache_discovered_path removed; paths are stored directly in save_file
-
     def get_statistics(self) -> Dict[str, Any]:
         """Get index statistics for debugging."""
         stats = {
@@ -1193,7 +1199,6 @@ def build_cli() -> argparse.ArgumentParser:
     p.add_argument("--io-workers", type=int, help="Number of I/O workers")
     p.add_argument("--batch-size", type=int, help="Streaming batch size")
     p.add_argument("--use-tar-streaming", action="store_true", help="Use streaming mode for large tars to reduce memory")
-    # removed tar-major option; tar-major extraction is no longer supported
 
     # Additional tuning options
     p.add_argument(
@@ -1295,8 +1300,6 @@ def apply_cli_overrides(args: argparse.Namespace, cfg: Config) -> None:
     if hasattr(args, 'use_tar_streaming') and args.use_tar_streaming:
         cfg.use_tar_streaming = True
 
-    # tar-major batching mode removed; always use simple streaming extraction
-
     # New configurable fields
     if hasattr(args, 'progress_update_interval') and args.progress_update_interval is not None:
         cfg.progress_update_interval = args.progress_update_interval
@@ -1674,168 +1677,6 @@ def process_extractions_streaming(metadata_stream: Iterator[Dict[str, Any]],
             logging.error(f"⚠️ WARNING: {len(pending_extractions)} files were not processed!")
 
 
-def process_extractions_tar_major(metadata_stream: Iterator[Dict[str, Any]],
-                                  cfg: Config, dest_dir: Path,
-                                  tar_index,
-                                  stop_handler: Optional[Any] = None) -> None:
-    """Accumulate rows per tar and process each tar in chunky batches to avoid open/close thrash."""
-    # Initialize sharding
-    sharding = DirectorySharding(dest_dir, cfg.files_per_shard)
-
-    # Initialize JSON writer if per-image JSON output is enabled
-    json_writer: Optional[Union[BatchedJSONWriter, AsyncJSONWriter]] = None
-    if cfg.per_image_json:
-        if cfg.json_writer_workers and cfg.json_writer_workers > 0:
-            json_writer = AsyncJSONWriter(workers=cfg.json_writer_workers, enable_fsync=cfg.enable_fsync)
-        else:
-            json_writer = BatchedJSONWriter(flush_interval=cfg.json_flush_interval, batch_size=1000, enable_fsync=cfg.enable_fsync)
-
-    # Create progress tracker for per-image completion tracking
-    progress_tracker = ValidatingProgressTracker(dest_dir / "progress.json",
-                                                 update_interval=cfg.progress_update_interval,
-                                                 enable_fsync=cfg.enable_fsync)
-
-    # Optional validation at start, to clean up or scan existing files
-    if cfg.validate_on_start:
-        if cfg.full_scan:
-            progress_tracker.full_filesystem_scan(sharding)
-        else:
-            progress_tracker.validate_files(sharding, auto_clean=True)
-
-    # Ensure safe shutdown flush: register hooks on stop_handler
-    if stop_handler:
-        if json_writer:
-            stop_handler.add_flush_hook(lambda: json_writer.close())
-        stop_handler.add_flush_hook(lambda: progress_tracker.save_final())
-
-    # Stats for reporting extraction outcomes
-    stats: Dict[str, Any] = {
-        'extracted': 0, 'failed': 0, 'skipped': 0, 'not_found': 0,
-        'not_found_ids': deque(maxlen=cfg.not_found_max_sample),
-        'total_bytes_written': 0, 'write_time': 0.0
-    }
-    stats_lock = threading.Lock()
-    reporter = ProgressReporter(interval=10.0)
-
-    logging.info("🔄 Starting TAR-major extraction…")
-
-    # Accumulate rows per tar; each key is tar_name mapping to list of (image_id, filename, metadata)
-    buckets: Dict[str, List[Tuple[int, str, Dict[str, Any]]]] = defaultdict(list)
-    # Determine threshold for flushing a tar bucket; ensure at least 50 to reduce thrashing
-    target_batch = max(50, cfg.batch_size)
-
-    # Helper to process a single tar once enough rows accumulate
-    def flush_tar(tar_name: str) -> None:
-        items = buckets.get(tar_name, [])
-        if not items:
-            return
-        # Process the tar via existing handler; this writes files and updates stats
-        process_single_tar(cfg, tar_name, items, sharding, progress_tracker, json_writer,
-                           tar_index, stats, stats_lock)
-        # Clear the bucket for this tar
-        buckets[tar_name].clear()
-        # Periodically report progress
-        if reporter.should_report():
-            reporter.report(stats)
-
-    # Iterate through metadata rows, bucketizing by tar
-    for row in metadata_stream:
-        # Allow for graceful shutdown via stop_handler
-        if stop_handler and stop_handler.should_stop():
-            logging.info("🛑 Stopping due to interrupt…")
-            break
-
-        image_id = row[cfg.id_col]
-
-        # Skip if already completed or exists in destination
-        if progress_tracker.is_completed(image_id):
-            with stats_lock:
-                stats['skipped'] += 1
-            continue
-        exists, _ = sharding.file_exists(image_id)
-        if exists:
-            progress_tracker.mark_completed(image_id)
-            with stats_lock:
-                stats['skipped'] += 1
-            continue
-
-        # Determine which tar and filename contain this image
-        file_url = row.get(cfg.file_path_col, "") if cfg.file_path_col else ""
-        tar_info = tar_index.find_image(image_id, file_url, row.get(cfg.md5_col))
-        if not tar_info:
-            with stats_lock:
-                stats['not_found'] += 1
-                stats['not_found_ids'].append(image_id)
-            continue
-
-        tar_name, filename = tar_info
-        buckets[tar_name].append((image_id, filename, row))
-
-        # If this tar bucket has reached the threshold, flush it
-        if len(buckets[tar_name]) >= target_batch:
-            flush_tar(tar_name)
-
-    # Final flush for any remaining buckets once streaming is finished
-    for tar_name, items in list(buckets.items()):
-        if items:
-            logging.info(f"📦 Finalizing {tar_name} with {len(items)} files…")
-            flush_tar(tar_name)
-
-    # Final reporting and cleanup
-    reporter.report(stats, force=True)
-    if json_writer:
-        json_writer.close()
-    progress_tracker.save_final()
+
 def process_extractions_simple(metadata_stream: Iterator[Dict[str, Any]],
                               cfg: Config, dest_dir: Path,
                               tar_index,
                               stop_handler: Optional[Any] = None) -> None:
     """Process extractions - now uses streaming mode for better memory efficiency."""
-    # Delegate to the streaming implementation for improved memory usage
     logging.info("🔄 Using streaming extraction mode for better memory efficiency...")
     process_extractions_streaming(metadata_stream, cfg, dest_dir, tar_index, stop_handler)
 
@@ -2094,8 +1935,7 @@ def main() -> None:
         
         # Process extractions
         if cfg.extract_images:
-            # Always use the streaming extraction; tar-major mode has been removed
             process_extractions_simple(metadata_stream, cfg, out_dir, tar_index, stop_handler)
         else:
             logging.info("🔄 Filtering metadata only (extraction disabled)")
             count = sum(1 for _ in metadata_stream)
