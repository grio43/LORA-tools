--- a/Dambooru2024_local.py
+++ b/Dambooru2024_local.py
@@ -559,11 +559,11 @@ def detect_metadata_structure(path: Path, cfg: Config) -> None:
     """Auto-detect metadata structure and update config accordingly."""
     logging.info("üîç Detecting metadata structure...")
     
     # First check with pandas to see if ID is in index
     try:
         import pandas as pd
-        pdf = pd.read_parquet(str(path), nrows=5)
+        pdf = pd.read_parquet(str(path), engine='pyarrow').head(5)
         
         # Check if ID is in the index
         if pdf.index.name == 'id' or (pdf.index.dtype in ['int64', 'int32'] and 'id' not in pdf.columns):
             logging.info("üìä ID column is in the DataFrame index, will handle accordingly")
             cfg.id_in_index = True
@@ -589,7 +589,12 @@ def detect_metadata_structure(path: Path, cfg: Config) -> None:
     # Show sample data for debugging
     lf = pl.scan_parquet(str(path))
-    sample = lf.head(2).collect()
+    try:
+        sample = lf.head(2).collect()
+    except Exception as e:
+        logging.warning(f"‚ö†Ô∏è Could not collect sample: {e}")
+        # Try without streaming
+        sample = pl.read_parquet(str(path), n_rows=2)
     logging.info(f"üìã Available columns: {sample.columns[:10]}...")
     logging.info(f"üìã Sample row:\n{sample.head(1)}")
 
@@ -614,6 +619,7 @@ def stream_filtered_metadata(path: Path, cfg: Config, stop_handler: Optional[So
                 pdf = pdf.rename(columns={'index': 'id'})
         # Convert to polars
         df_full = pl.from_pandas(pdf)
         lf = df_full.lazy()
+        logging.info(f"üìä Loaded {len(df_full):,} total rows from pandas conversion")
     
     # Apply columns selection
     cols_to_load: set[str] = set()
@@ -854,7 +860,11 @@ def stream_filtered_metadata(path: Path, cfg: Config, stop_handler: Optional[So
     # Get available columns
-    available_cols = set(lf.schema.keys())
+    try:
+        available_cols = set(lf.collect_schema().keys())
+    except Exception:
+        # Fallback: collect small sample to get schema
+        available_cols = set(lf.head(1).collect().columns)
     final_cols = list(cols_to_load.intersection(available_cols))
 
     # md5 is optional
@@ -917,18 +927,33 @@ def stream_filtered_metadata(path: Path, cfg: Config, stop_handler: Optional[So
     
     try:
         logging.info(f"üî• Starting streaming collection with batch size {cfg.batch_size}")
-
-        # Collect the filtered data with streaming.
+        
+        # Try different collection strategies
+        df = None
         try:
-            df = lf.collect(engine="streaming")  # new API
-        except TypeError:
-            df = lf.collect(streaming=True)      # fallback for older Polars
-        except Exception as e:
-            logging.error(f"‚ùå Failed to collect with streaming: {e}")
-            logging.info("üîÑ Falling back to regular collect...")
-            df = lf.collect()
+            # First try: streaming with new API
+            df = lf.collect(streaming=True)
+            logging.info("‚úÖ Using streaming collection")
+        except Exception as e1:
+            logging.warning(f"‚ö†Ô∏è Streaming collection failed: {e1}")
+            try:
+                # Second try: regular collect
+                logging.info("üîÑ Trying regular collection...")
+                df = lf.collect()
+                logging.info("‚úÖ Using regular collection")
+            except Exception as e2:
+                logging.warning(f"‚ö†Ô∏è Regular collection failed: {e2}")
+                # Final fallback: read directly with chunking
+                logging.info("üîÑ Using direct parquet read with chunking...")
+                df = pl.read_parquet(str(path))
+                # Apply same column selection and filters
+                if final_cols:
+                    df = df.select(final_cols)
+                filter_expr = build_polars_filter_expr(cfg)
+                df = df.filter(filter_expr)
+                logging.info("‚úÖ Using direct read")
 
         total_rows = len(df)
         logging.info(f"üìä Filtered metadata contains {total_rows:,} matching images")
         
         if total_rows == 0:
             logging.warning("‚ö†Ô∏è No images match the filter criteria!")
@@ -936,10 +961,17 @@ def stream_filtered_metadata(path: Path, cfg: Config, stop_handler: Optional[So
 
         # Iterate over the collected DataFrame in batches.
         batch_count = 0
         total_yielded = 0
-        for batch_df in df.iter_slices(cfg.batch_size):
+        
+        # Use iter_slices for memory efficiency
+        slice_size = min(cfg.batch_size, 10000)  # Cap slice size for safety
+        
+        for batch_df in df.iter_slices(slice_size):
             batch_count += 1
-            if batch_count % 10 == 0:
-                logging.info(f"üìä Processing batch {batch_count} ({total_yielded:,}/{total_rows:,} items)...")
+            
+            # More frequent progress updates for large datasets
+            if batch_count % 5 == 0 or total_yielded == 0:
+                progress_pct = (total_yielded / total_rows * 100) if total_rows > 0 else 0
+                logging.info(f"üìä Processing batch {batch_count} ({total_yielded:,}/{total_rows:,} items - {progress_pct:.1f}%)...")
 
             # Check for stop signal
             if stop_handler and stop_handler.should_stop():
@@ -2041,6 +2073,18 @@ def main() -> None:
     
     out_dir.mkdir(parents=True, exist_ok=True)
     
+    # Check Polars version and settings
+    try:
+        import polars as pl
+        pl_version = pl.__version__
+        logging.info(f"üì¶ Using Polars version: {pl_version}")
+        
+        # Set Polars to use all available cores
+        import os
+        os.environ["POLARS_MAX_THREADS"] = str(os.cpu_count())
+    except Exception as e:
+        logging.warning(f"‚ö†Ô∏è Could not check Polars version: {e}")
+    
     # Auto-detect metadata structure
     detect_metadata_structure(meta_path, cfg)
