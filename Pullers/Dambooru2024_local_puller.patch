--- a/Dambooru2024_local.py
+++ b/Dambooru2024_local.py
@@ -128,6 +128,9 @@
 
     # Memory management for large tar files
     use_tar_streaming: bool = False  # Use 'r|' mode for very large tars to reduce memory
+    # Process mode: accumulate per-tar to avoid thrash
+    tar_major: bool = False  # If True, batch by tar instead of switching constantly
+
 
 # ---------------------------------------------------------------------------
 # Directory Sharding
@@ -1113,6 +1116,7 @@
     p.add_argument("--io-workers", type=int, help="Number of I/O workers")
     p.add_argument("--batch-size", type=int, help="Streaming batch size")
     p.add_argument("--use-tar-streaming", action="store_true", help="Use streaming mode for large tars to reduce memory")
+    p.add_argument("--tar-major", action="store_true", help="Batch by tar (accumulate and process per-tar to reduce open/close thrash)")
 
     # Additional tuning options
     p.add_argument(
@@ -1222,6 +1226,8 @@
         cfg.io_workers = args.io_workers
     if args.batch_size is not None:
         cfg.batch_size = args.batch_size
     if hasattr(args, 'use_tar_streaming') and args.use_tar_streaming:
         cfg.use_tar_streaming = True
+    if hasattr(args, 'tar_major') and args.tar_major:
+        cfg.tar_major = True
 
     # New configurable fields
     if hasattr(args, 'progress_update_interval') and args.progress_update_interval is not None:
@@ -1558,6 +1564,105 @@
         # Log if we had any unprocessed files
         if pending_extractions:
             logging.error(f"âš ï¸ WARNING: {len(pending_extractions)} files were not processed!")
+
+def process_extractions_tar_major(metadata_stream: Iterator[Dict[str, Any]],
+                                  cfg: Config, dest_dir: Path,
+                                  tar_index,
+                                  stop_handler: Optional[Any] = None) -> None:
+    """Accumulate rows per tar and process each tar in chunky batches to avoid open/close thrash."""
+    sharding = DirectorySharding(dest_dir, cfg.files_per_shard)
+
+    # Initialize JSON writer
+    json_writer: Optional[Union[BatchedJSONWriter, AsyncJSONWriter]] = None
+    if cfg.per_image_json:
+        if cfg.json_writer_workers and cfg.json_writer_workers > 0:
+            json_writer = AsyncJSONWriter(workers=cfg.json_writer_workers, enable_fsync=cfg.enable_fsync)
+        else:
+            json_writer = BatchedJSONWriter(flush_interval=cfg.json_flush_interval, batch_size=1000, enable_fsync=cfg.enable_fsync)
+
+    # Progress tracker
+    progress_tracker = ValidatingProgressTracker(dest_dir / "progress.json",
+                                                 update_interval=cfg.progress_update_interval,
+                                                 enable_fsync=cfg.enable_fsync)
+
+    # Optional validation at start
+    if cfg.validate_on_start:
+        if cfg.full_scan:
+            progress_tracker.full_filesystem_scan(sharding)
+        else:
+            progress_tracker.validate_files(sharding, auto_clean=True)
+
+    # Ensure safe shutdown flush
+    if stop_handler:
+        if json_writer:
+            stop_handler.add_flush_hook(lambda: json_writer.close())
+        stop_handler.add_flush_hook(lambda: progress_tracker.save_final())
+
+    stats: Dict[str, Any] = {
+        'extracted': 0, 'failed': 0, 'skipped': 0, 'not_found': 0,
+        'not_found_ids': deque(maxlen=cfg.not_found_max_sample),
+        'total_bytes_written': 0, 'write_time': 0.0
+    }
+    stats_lock = threading.Lock()
+    reporter = ProgressReporter(interval=10.0)
+
+    logging.info("ðŸ”„ Starting TAR-major extractionâ€¦")
+
+    # Accumulate rows per tar
+    buckets: Dict[str, List[Tuple[int, str, Dict[str, Any]]]] = defaultdict(list)
+    target_batch = max(50, cfg.batch_size)
+
+    def flush_tar(tar_name: str):
+        items = buckets.get(tar_name, [])
+        if not items:
+            return
+        process_single_tar(cfg, tar_name, items, sharding, progress_tracker, json_writer,
+                           tar_index, stats, stats_lock)
+        buckets[tar_name].clear()
+        if reporter.should_report():
+            reporter.report(stats)
+
+    for row in metadata_stream:
+        if stop_handler and stop_handler.should_stop():
+            logging.info("ðŸ›‘ Stopping due to interruptâ€¦")
+            break
+
+        image_id = row[cfg.id_col]
+
+        # Skip if already completed or exists
+        if progress_tracker.is_completed(image_id):
+            with stats_lock:
+                stats['skipped'] += 1
+            continue
+        exists, _ = sharding.file_exists(image_id)
+        if exists:
+            progress_tracker.mark_completed(image_id)
+            with stats_lock:
+                stats['skipped'] += 1
+            continue
+
+        # Find tar + internal name
+        file_url = row.get(cfg.file_path_col, "")
+        tar_info = tar_index.find_image(image_id, file_url)
+        if not tar_info:
+            with stats_lock:
+                stats['not_found'] += 1
+                stats['not_found_ids'].append(image_id)
+            continue
+
+        tar_name, filename = tar_info
+        buckets[tar_name].append((image_id, filename, row))
+
+        # Flush when this tar bucket reaches threshold
+        if len(buckets[tar_name]) >= target_batch:
+            flush_tar(tar_name)
+
+    # Final flush for all remaining tars
+    for tar_name, items in list(buckets.items()):
+        if items:
+            logging.info(f"ðŸ“¦ Finalizing {tar_name} with {len(items)} filesâ€¦")
+            flush_tar(tar_name)
+
+    reporter.report(stats, force=True)
+    if json_writer:
+        json_writer.close()
+    progress_tracker.save_final()
 def process_extractions_simple(metadata_stream: Iterator[Dict[str, Any]],
                               cfg: Config, dest_dir: Path,
                               tar_index,
@@ -1814,7 +1919,10 @@
         
         # Process extractions
         if cfg.extract_images:
-            process_extractions_simple(metadata_stream, cfg, out_dir, tar_index, stop_handler)
+            process_extractions_tar_major(
+                metadata_stream, cfg, out_dir, tar_index, stop_handler
+            ) if getattr(cfg, "tar_major", False) else process_extractions_simple(
+                metadata_stream, cfg, out_dir, tar_index, stop_handler
+            )
         else:
             logging.info("ðŸ“„ Filtering metadata only (extraction disabled)")
             count = sum(1 for _ in metadata_stream)
             logging.info(f"âœ… Found {count:,} images matching criteria")

