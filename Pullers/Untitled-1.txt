Help me upgrade Danbooru, provide me before and after snippets to upgrade Danbooru. You are given rule34 as a point as refrence only.

## **Critical Robustness Features Missing in Danbooru:**

### **1. Progress Tracking & Resumability**
**Rule34 has:**
```python
class ProgressTracker:
    - Persistent JSON file tracking completed downloads
    - Atomic updates every N downloads
    - Can resume interrupted sessions
    - Survives crashes/interruptions
```
**Danbooru lacks:** Any resume capability - if interrupted, must restart from beginning

### **2. Directory Sharding**
**Rule34 has:**
```python
class DirectorySharding:
    - Distributes files across subdirectories (5000 files each)
    - O(1) file existence checks
    - Prevents filesystem slowdown with millions of files
    - Shorter file paths
```
**Danbooru lacks:** All files go to one directory - becomes very slow with many files

### **3. Memory-Efficient Streaming**
**Rule34 has:**
```python
# Constant memory usage regardless of dataset size
lf.collect(streaming=True).iter_slices(batch_size)
# Processes in chunks of 1000 rows
```
**Danbooru has:**
```python
# Loads entire filtered dataset into memory
df = pd.read_parquet(path, columns=final_cols)
df_sub = df[mask]  # All in memory
```

### **4. Graceful Interruption Handling**
**Rule34 has:**
```python
class SoftStopHandler:
    - First Ctrl+C: Finishes current batch gracefully
    - Second Ctrl+C: Force exit
    - Saves progress before exiting
```
**Danbooru lacks:** Hard stop only - loses all progress

### **5. Individual File Error Handling**
**Rule34:** 
- Each download in try/except
- Failures logged but don't stop process
- Returns success/failure per file

**Danbooru:** 
- Batch operation - unclear failure handling
- Likely stops on critical errors

### **6. Verified Completion**
**Rule34:**
```python
# Only marks complete AFTER successful download
pool.batch_download_to_directory([id])
# Verify success
progress_tracker.mark_completed(image_id)
# Only then write metadata
```
**Danbooru:** No verification - assumes all downloads succeed

### **7. Batched JSON Writing**
**Rule34 has:**
```python
class BatchedJSONWriter:
    - Buffers writes in memory
    - Flushes periodically or when buffer full
    - Reduces I/O operations significantly
    - Atomic writes with temp files
```
**Danbooru:** Writes each JSON immediately (inefficient I/O)

### **8. Bounded Concurrency Control**
**Rule34:**
```python
max_outstanding = workers * 10  # Bounded queue
# Prevents memory overflow with large datasets
```
**Danbooru:** Submits entire dataset at once - could cause memory issues

### **9. Thread-Safe HF Cache**
**Rule34:** Monkey-patches HF cache for thread safety with concurrent downloads
**Danbooru:** Potential race conditions with high worker counts

### **10. Atomic File Operations**
**Rule34:** All file writes use temp file + rename pattern
**Danbooru:** Direct writes - can leave corrupted files if interrupted

## **Quick Wins to Implement First:**

### **Priority 1 - Progress Tracking (Biggest Impact)**
```python
# Minimal implementation
class SimpleProgressTracker:
    def __init__(self, progress_file):
        self.progress_file = progress_file
        self.completed = set()
        if progress_file.exists():
            with open(progress_file) as f:
                self.completed = set(json.load(f))
    
    def mark_completed(self, id):
        self.completed.add(id)
        if len(self.completed) % 100 == 0:
            self.save()
    
    def save(self):
        with open(f"{self.progress_file}.tmp", 'w') as f:
            json.dump(list(self.completed), f)
        Path(f"{self.progress_file}.tmp").rename(self.progress_file)
```

### **Priority 2 - Use Polars Streaming**
```python
# Replace pandas with polars streaming
import polars as pl

# Instead of:
df = pd.read_parquet(path)

# Use:
lf = pl.scan_parquet(path)
for batch in lf.collect(streaming=True).iter_slices(1000):
    # Process batch
```

### **Priority 3 - Individual Downloads with Error Handling**
```python
# Instead of one batch_download_to_directory call with all IDs
# Do individual downloads with verification:
for image_id in ids:
    try:
        pool.batch_download_to_directory([image_id], dst_dir, max_workers=1)
        progress_tracker.mark_completed(image_id)
    except Exception as e:
        log.error(f"Failed {image_id}: {e}")
        continue
```

### **Priority 4 - Directory Sharding**
```python
# Simple sharding
def get_shard_dir(base_dir, image_id, files_per_shard=5000):
    shard_num = image_id // files_per_shard
    shard_dir = base_dir / f"shard_{shard_num:05d}"
    shard_dir.mkdir(exist_ok=True)
    return shard_dir
```

These improvements would transform the Danbooru script from a simple batch downloader to a production-ready, resilient data collection tool that can handle interruptions, scale to millions of files, and provide clear progress tracking.